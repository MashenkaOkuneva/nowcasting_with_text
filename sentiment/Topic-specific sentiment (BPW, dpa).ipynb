{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0cfcea",
   "metadata": {},
   "source": [
    "# Sentiment-Adjusted Topic Series (BPW)\n",
    "\n",
    "In this notebook, I construct sentiment-adjusted topic series. For sentiment extraction, I use the German translation by [Bannier, Pauls, and Walter (BPW)](https://link.springer.com/article/10.1007/s11573-018-0914-8) of the widely recognized dictionary by [Loughran and McDonald (2011)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.2010.01625.x). \n",
    "\n",
    "The process involves identifying the 10 articles with the highest proportion of each topic for each day. I then calculate the average sentiment measure for these articles, which serves as the sentiment value for the topic on that day. Finally, I adjust the daily topic values by multiplying them with the corresponding sentiment measure.\n",
    "\n",
    "To begin, I load the datasets from Handelsblatt, SZ, Welt, and dpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d2dd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3336299\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>dpa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>dpa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>dpa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sperrfrist 1. Januar 1000 HBV fordert umfassen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>dpa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schamir weist Nahost-Äußerungen des neuen EG-P...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>dpa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  day  month  year source\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...    1      1  1991    dpa\n",
       "1  Welajati: Iran bleibt bei einem Krieg am Golf ...    1      1  1991    dpa\n",
       "2  Bush will offenbar seinen Außenminister erneut...    1      1  1991    dpa\n",
       "3  Sperrfrist 1. Januar 1000 HBV fordert umfassen...    1      1  1991    dpa\n",
       "4  Schamir weist Nahost-Äußerungen des neuen EG-P...    1      1  1991    dpa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Set the path variable to point to the 'newspaper_data_processing' directory.\n",
    "path = os.getcwd().replace('\\\\nowcasting_with_text\\\\sentiment', '\\\\newspaper_data_processing')\n",
    "\n",
    "# Load pre-processed 'dpa' dataset from a CSV file.\n",
    "dpa = pd.read_csv(path + '\\\\dpa\\\\' + 'dpa_prepro_final.csv', encoding = 'utf-8', sep=';', index_col = 0,  keep_default_na=False,\n",
    "                   dtype = {'rubrics': 'str', \n",
    "                            'source': 'str',\n",
    "                            'keywords': 'str',\n",
    "                            'title': 'str',\n",
    "                            'city': 'str',\n",
    "                            'genre': 'str',\n",
    "                            'wordcount': 'str'},\n",
    "                  converters = {'paragraphs': literal_eval})\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "dpa = dpa[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Add a new column 'source' with the value 'dpa' for all rows\n",
    "dpa['source'] = 'dpa'\n",
    "\n",
    "# Load pre-processed 'SZ' dataset from a CSV file.\n",
    "sz = pd.read_csv(path + '\\\\SZ\\\\' + 'sz_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'newspaper_2': 'str',\n",
    "                                                                                                 'quelle_texts': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "sz.page = sz.page.fillna('')\n",
    "sz.newspaper = sz.newspaper.fillna('')\n",
    "sz.newspaper_2 = sz.newspaper_2.fillna('')\n",
    "sz.rubrics = sz.rubrics.fillna('')\n",
    "sz.quelle_texts = sz.quelle_texts.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "sz = sz[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Add a new column 'source' with the value 'sz' for all rows\n",
    "sz['source'] = 'sz'\n",
    "\n",
    "# Load pre-processed 'Handelsblatt' dataset from a CSV file.\n",
    "hb = pd.read_csv(path + '\\\\Handelsblatt\\\\' + 'hb_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'kicker': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'series_title': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "hb.page = hb.page.fillna('')\n",
    "hb.series_title = hb.series_title.fillna('')\n",
    "hb.kicker = hb.kicker.fillna('')\n",
    "hb.rubrics = hb.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "hb = hb[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Add a new column 'source' with the value 'hb' for all rows\n",
    "hb['source'] = 'hb'\n",
    "\n",
    "# Load pre-processed 'Welt' dataset from a CSV file.\n",
    "welt = pd.read_csv(path + '\\\\Welt\\\\' + 'welt_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'rubrics': 'str',\n",
    "                                                                                                 'title': 'str'})\n",
    "welt.title = welt.title.fillna('')\n",
    "welt.rubrics = welt.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "welt = welt[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Add a new column 'source' with the value 'welt' for all rows\n",
    "welt['source'] = 'welt'\n",
    "\n",
    "# Concatenate the 'dpa', 'sz', 'hb', and 'welt' DataFrames into a single DataFrame 'data'.\n",
    "data = pd.concat([dpa, sz, hb, welt])\n",
    "\n",
    "# The number of articles in the final dataset.\n",
    "print(len(data))\n",
    "\n",
    "# Sort the data in chronological order.\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "# Reset the index of the DataFrame\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69b80d",
   "metadata": {},
   "source": [
    "Next, I calculate the sentiment measure for each article as the difference between the number of positive and negative words, divided by the total number of words in the article.\n",
    "\n",
    "To do this, I first load the BPW dictionary and create two lists: one containing negative terms and another containing positive terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b68382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbau', 'abbauen', 'abbauend', 'abbauende', 'abbauendem']\n",
      "['adäquat', 'adäquate', 'adäquatem', 'adäquaten', 'adäquater']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read an Excel file, transform an output into a list\n",
    "bpw_neg = list(pd.read_excel('BPW_Dictionary.xlsx', sheet_name='NEG_BPW', header=None).iloc[:,0]) \n",
    "bpw_pos = list(pd.read_excel('BPW_Dictionary.xlsx', sheet_name='POS_BPW', header=None).iloc[:,0])\n",
    "\n",
    "# Convert boolean value back to its intended string form\n",
    "bpw_neg = ['falsch' if word is False else word for word in bpw_neg]\n",
    "\n",
    "print(bpw_neg[:5])\n",
    "print(bpw_pos[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5abec",
   "metadata": {},
   "source": [
    "I then calculate the number of negative words in each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e278958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:53.684749\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import count_words_chunk\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Number of cores to use\n",
    "NUM_CORE = mp.cpu_count() - 4 \n",
    "\n",
    "# Split data into chunks for parallel processing\n",
    "chunk_size = len(data.texts) // NUM_CORE + 1 \n",
    "text_chunks = [data.texts[i:i + chunk_size] for i in range(0, len(data.texts), chunk_size)]\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "    # Process each chunk in parallel\n",
    "    nw_results = pool.starmap(count_words_chunk.count_words_chunk, [(chunk, bpw_neg) for chunk in text_chunks])\n",
    "\n",
    "    # Close and join the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine results from all chunks\n",
    "    negative_counts = np.concatenate(nw_results)\n",
    "\n",
    "print(datetime.now() - startTime)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766919f",
   "metadata": {},
   "source": [
    "I proceed to calculate the number of positive words in each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf15b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:42.586572\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "    # Process each chunk in parallel\n",
    "    pw_results = pool.starmap(count_words_chunk.count_words_chunk, [(chunk, bpw_pos) for chunk in text_chunks])\n",
    "\n",
    "    # Close and join the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine results from all chunks\n",
    "    positive_counts = np.concatenate(pw_results)\n",
    "\n",
    "print(datetime.now() - startTime)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d022bb3",
   "metadata": {},
   "source": [
    "The final statistic required for calculating sentiment is the total number of words in each article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286c7082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:37.665531\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../uncertainty\")\n",
    "\n",
    "startTime = datetime.now() \n",
    "\n",
    "# Import the function calculating the number of words in a text\n",
    "import count_words_mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955718f9",
   "metadata": {},
   "source": [
    "Finally, I calculate the sentiment for each article as the proportion of positive words minus the proportion of negative words, relative to the total word count. This value is then added as a new column to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df2aeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert count_results to a NumPy array for element-wise operations\n",
    "count_results = np.array(count_results)\n",
    "\n",
    "# Calculate sentiment: (positive_counts - negative_counts) / count_results\n",
    "sentiment_measure = np.divide(\n",
    "    positive_counts - negative_counts, \n",
    "    count_results, \n",
    "    out=np.zeros_like(negative_counts, dtype=float), \n",
    "    where=count_results != 0\n",
    ")\n",
    "\n",
    "# Add the sentiment to the DataFrame\n",
    "data['sentiment'] = sentiment_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9666cf",
   "metadata": {},
   "source": [
    "Now, I incorporate the topic distributions for each article, which were previously computed using the Latent Dirichlet Allocation (LDA) algorithm in the notebook titled `Topic Model Estimation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b0fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path variable to point to the 'topics' directory.\n",
    "path = os.getcwd().replace('\\\\sentiment', '\\\\topics')\n",
    "\n",
    "# Load the article topics from a CSV file\n",
    "article_topics = pd.read_csv(path + '\\\\article_topic.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "# Merge the `data` DataFrame with the `article_topics` DataFrame\n",
    "data = pd.concat([data, article_topics], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d258a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where source is 'dpa'\n",
    "data = data[data['source'] == 'dpa']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30adb1",
   "metadata": {},
   "source": [
    "I define a function, `get_average_sentiment`, which calculates the average sentiment measure for each topic on a given day. This function selects a specified number of articles with the highest proportions for each topic and computes the average sentiment across these articles. The `calculate_average_sentiment` function applies this calculation to the entire dataset by grouping the data by date and processing each group in parallel using multiprocessing. The results are then combined into a single pandas DataFrame, with the topics as columns and the dates as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602ec4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_average_sentiment import get_average_sentiment\n",
    "\n",
    "# Convert 'year', 'month', 'day' to datetime\n",
    "data['date'] = pd.to_datetime(data[['year', 'month', 'day']])\n",
    "\n",
    "def calculate_average_sentiment(data, n_articles):\n",
    "    \"\"\"\n",
    "    Function to calculate average sentiment for topics based on a specified number of articles.\n",
    "    \"\"\"\n",
    "    # Group data by 'date' to ensure each day stays together\n",
    "    grouped_by_date = [group for _, group in data.groupby('date')]\n",
    "    \n",
    "    # Prepare arguments for starmap (pair each group with the value of n_articles)\n",
    "    args = [(group, n_articles) for group in grouped_by_date]\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        # Create a multiprocessing pool\n",
    "        pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "        # Process each group (one day of data) in parallel\n",
    "        results = pool.starmap(get_average_sentiment, args)\n",
    "\n",
    "        # Concatenate the results into a single DataFrame\n",
    "        daily_average_sentiment = pd.concat(results)\n",
    "\n",
    "        # Close and join the pool\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    return daily_average_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c2eb792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:12.641810\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() \n",
    "\n",
    "# Generate average sentiment for 5, 10, and 15 articles\n",
    "daily_average_sentiment_5 = calculate_average_sentiment(data, n_articles=5)\n",
    "daily_average_sentiment_10 = calculate_average_sentiment(data, n_articles=10)\n",
    "daily_average_sentiment_15 = calculate_average_sentiment(data, n_articles=15)\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cda1b8",
   "metadata": {},
   "source": [
    "Now I am going to load the daily topics and adjust them using the DataFrame `daily_average_sentiment_10`. The result is a dataframe where topic distributions are multiplied with the average sentiment of each topic for a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c1c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the daily topics from a CSV file\n",
    "daily_topics = pd.read_csv(path + '\\\\daily_topics_dpa.csv', encoding='utf-8')\n",
    "\n",
    "# Convert year, month, and day into a single date column\n",
    "daily_topics['date'] = pd.to_datetime(daily_topics[['year','month','day']])\n",
    "daily_topics.drop(columns=['year', 'month', 'day'], inplace=True)\n",
    "\n",
    "# Now, set 'date' as index\n",
    "daily_topics.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac1e7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment adjustment to the daily topics\n",
    "sentiment_adjusted_daily_topics = daily_topics.multiply(daily_average_sentiment_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472e6d7",
   "metadata": {},
   "source": [
    "I iterate over each topic and generate a graph comparing the original and sentiment-adjusted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c72ca519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "\n",
    "# Create a directory to save the plots\n",
    "os.makedirs('topics_plots_dpa', exist_ok=True)\n",
    "\n",
    "# Define the shaded areas for recessions\n",
    "recessions = [\n",
    "    (\"1992-01-01\", \"1993-12-31\"),  # Post-reunification recession\n",
    "    (\"2001-01-01\", \"2001-12-31\"),  # Dot-com recession\n",
    "    (\"2008-01-01\", \"2009-12-31\"),  # Great Recession\n",
    "    (\"2011-01-01\", \"2013-12-31\")   # European sovereign debt crisis\n",
    "]\n",
    "\n",
    "# Calculate the 180-day rolling mean for each series\n",
    "daily_topics_rm = daily_topics.rolling(window=180).mean()\n",
    "sentiment_adjusted_daily_topics_10_rm = sentiment_adjusted_daily_topics.rolling(window=180).mean()\n",
    "\n",
    "# Iterate over each topic\n",
    "for i in range(daily_topics.shape[1]):\n",
    "    # Generate the plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original topics on the primary y-axis\n",
    "    ax1.plot(daily_topics_rm.index, daily_topics_rm.iloc[:, i], label='Original Topic', color='black')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Original Topic Proportion', color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Add shaded areas for recessions\n",
    "    for start, end in recessions:\n",
    "        ax1.axvspan(pd.to_datetime(start), pd.to_datetime(end), color='grey', alpha=0.3)\n",
    "    \n",
    "    # Create a secondary y-axis for the sentiment-adjusted topics\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(sentiment_adjusted_daily_topics_10_rm.index, sentiment_adjusted_daily_topics_10_rm.iloc[:, i], label='Sentiment-Adjusted Topic', linestyle='--')\n",
    "    ax2.set_ylabel('Sentiment-Adjusted Topic')\n",
    "    ax2.tick_params(axis='y')\n",
    "    \n",
    "    # Add title and legends\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Format the x-axis to show every year\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Save the plot in the 'topics_plots_dpa' directory\n",
    "    plt.savefig('topics_plots_dpa/Topic_' + str(i) + '.png')\n",
    "    \n",
    "    # Clear the current figure to free memory\n",
    "    plt.clf()\n",
    "    \n",
    "    # Close the current figure to free memory\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039029c",
   "metadata": {},
   "source": [
    "To check the robustness of my results to the choice of the number of top articles used to determine sentiment, I compare the sentiment-adjusted topics for 5, 10, and 15 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c67fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the plots\n",
    "os.makedirs('topics_plots_number_of_articles_dpa', exist_ok=True)\n",
    "\n",
    "# Calculate the 180-day rolling mean for each series\n",
    "sentiment_adjusted_daily_topics_5_rm = daily_topics.multiply(daily_average_sentiment_5).rolling(window=180).mean()\n",
    "sentiment_adjusted_daily_topics_15_rm = daily_topics.multiply(daily_average_sentiment_15).rolling(window=180).mean()\n",
    "\n",
    "# Iterate over each topic\n",
    "for i in range(daily_topics.shape[1]):\n",
    "    # Generate the plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original topics on the primary y-axis\n",
    "    ax1.plot(daily_topics_rm.index, daily_topics_rm.iloc[:, i], label='Original Topic', color='black')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Original Topic Proportion', color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Add shaded areas for recessions\n",
    "    for start, end in recessions:\n",
    "        ax1.axvspan(pd.to_datetime(start), pd.to_datetime(end), color='grey', alpha=0.3)\n",
    "    \n",
    "    # Create a secondary y-axis for the sentiment-adjusted topics\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(sentiment_adjusted_daily_topics_5_rm.index, sentiment_adjusted_daily_topics_5_rm.iloc[:, i], label='Sentiment-Adjusted (Top 5 Articles)', linestyle='--', color='blue')\n",
    "    ax2.plot(sentiment_adjusted_daily_topics_10_rm.index, sentiment_adjusted_daily_topics_10_rm.iloc[:, i], label='Sentiment-Adjusted (Top 10 Articles)', linestyle='-.', color='green')\n",
    "    ax2.plot(sentiment_adjusted_daily_topics_15_rm.index, sentiment_adjusted_daily_topics_15_rm.iloc[:, i], label='Sentiment-Adjusted (Top 15 Articles)', linestyle=':', color='red')\n",
    "    ax2.set_ylabel('Sentiment-Adjusted Topic')\n",
    "    ax2.tick_params(axis='y')\n",
    "    \n",
    "    # Add title and legends\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Format the x-axis to show every year\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Save the plot in the 'topics_plots_number_of_articles_dpa' directory\n",
    "    plt.savefig('topics_plots_number_of_articles_dpa/Topic_' + str(i) + '.png')\n",
    "    \n",
    "    # Clear the current figure to free memory\n",
    "    plt.clf()\n",
    "    \n",
    "    # Close the current figure to free memory\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49a48cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "sentiment_adjusted_daily_topics = sentiment_adjusted_daily_topics.reset_index()\n",
    "\n",
    "# Create 'year', 'month', and 'day' columns\n",
    "sentiment_adjusted_daily_topics['year'] = sentiment_adjusted_daily_topics['date'].dt.year\n",
    "sentiment_adjusted_daily_topics['month'] = sentiment_adjusted_daily_topics['date'].dt.month\n",
    "sentiment_adjusted_daily_topics['day'] = sentiment_adjusted_daily_topics['date'].dt.day\n",
    "\n",
    "# Drop the old 'index' column which holds the date\n",
    "sentiment_adjusted_daily_topics = sentiment_adjusted_daily_topics.drop(columns=['date'])\n",
    "\n",
    "# Reorder the columns to have 'year', 'month', 'day' as the first three columns\n",
    "cols = ['year', 'month', 'day'] + [col for col in sentiment_adjusted_daily_topics if col not in ['year', 'month', 'day']]\n",
    "sentiment_adjusted_daily_topics_format = sentiment_adjusted_daily_topics[cols]\n",
    "\n",
    "# Save sentiment-adjusted topics to a CSV file\n",
    "sentiment_adjusted_daily_topics_format.to_csv('sentiment_adjusted_daily_topics_dpa.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env_gpu",
   "language": "python",
   "name": "py3_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
