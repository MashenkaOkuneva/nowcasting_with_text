{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a58695d",
   "metadata": {},
   "source": [
    "# Sentiment-Adjusted Topic Series (SentiWS)\n",
    "\n",
    "In this notebook, I construct sentiment-adjusted topic series. For sentiment extraction, I use the SentiWS dictionary by [Remus, R., Quasthoff, U., and Heyer, G.](https://www.researchgate.net/publication/220746590_SentiWS_-_A_Publicly_Available_German-language_Resource_for_Sentiment_Analysis). \n",
    "\n",
    "The process involves identifying the 10 articles with the highest proportion of each topic for each day. I then calculate the average sentiment measure for these articles, which serves as the sentiment value for the topic on that day. Finally, I adjust the daily topic values by multiplying them with the corresponding sentiment measure.\n",
    "\n",
    "To begin, I load the datasets from Handelsblatt, SZ, Welt, and dpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc815f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3336299\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sperrfrist 1. Januar 1000 HBV fordert umfassen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schamir weist Nahost-Äußerungen des neuen EG-P...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  day  month  year\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...    1      1  1991\n",
       "1  Welajati: Iran bleibt bei einem Krieg am Golf ...    1      1  1991\n",
       "2  Bush will offenbar seinen Außenminister erneut...    1      1  1991\n",
       "3  Sperrfrist 1. Januar 1000 HBV fordert umfassen...    1      1  1991\n",
       "4  Schamir weist Nahost-Äußerungen des neuen EG-P...    1      1  1991"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Set the path variable to point to the 'newspaper_data_processing' directory.\n",
    "path = os.getcwd().replace('\\\\nowcasting_with_text\\\\sentiment', '\\\\newspaper_data_processing')\n",
    "\n",
    "# Load pre-processed 'dpa' dataset from a CSV file.\n",
    "dpa = pd.read_csv(path + '\\\\dpa\\\\' + 'dpa_prepro_final.csv', encoding = 'utf-8', sep=';', index_col = 0,  keep_default_na=False,\n",
    "                   dtype = {'rubrics': 'str', \n",
    "                            'source': 'str',\n",
    "                            'keywords': 'str',\n",
    "                            'title': 'str',\n",
    "                            'city': 'str',\n",
    "                            'genre': 'str',\n",
    "                            'wordcount': 'str'},\n",
    "                  converters = {'paragraphs': literal_eval})\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "dpa = dpa[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'SZ' dataset from a CSV file.\n",
    "sz = pd.read_csv(path + '\\\\SZ\\\\' + 'sz_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'newspaper_2': 'str',\n",
    "                                                                                                 'quelle_texts': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "sz.page = sz.page.fillna('')\n",
    "sz.newspaper = sz.newspaper.fillna('')\n",
    "sz.newspaper_2 = sz.newspaper_2.fillna('')\n",
    "sz.rubrics = sz.rubrics.fillna('')\n",
    "sz.quelle_texts = sz.quelle_texts.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "sz = sz[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Handelsblatt' dataset from a CSV file.\n",
    "hb = pd.read_csv(path + '\\\\Handelsblatt\\\\' + 'hb_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'kicker': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'series_title': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "hb.page = hb.page.fillna('')\n",
    "hb.series_title = hb.series_title.fillna('')\n",
    "hb.kicker = hb.kicker.fillna('')\n",
    "hb.rubrics = hb.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "hb = hb[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Welt' dataset from a CSV file.\n",
    "welt = pd.read_csv(path + '\\\\Welt\\\\' + 'welt_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'rubrics': 'str',\n",
    "                                                                                                 'title': 'str'})\n",
    "welt.title = welt.title.fillna('')\n",
    "welt.rubrics = welt.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "welt = welt[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Concatenate the 'dpa', 'sz', 'hb', and 'welt' DataFrames into a single DataFrame 'data'.\n",
    "data = pd.concat([dpa, sz, hb, welt])\n",
    "\n",
    "# The number of articles in the final dataset.\n",
    "print(len(data))\n",
    "\n",
    "# Sort the data in chronological order.\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "# Reset the index of the DataFrame\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36119e5f",
   "metadata": {},
   "source": [
    "Next, I calculate the sentiment measure for each article as the average sentiment score per word.\n",
    "\n",
    "To do this, I first load the SentiWs files into a single dictionary mapping every base word and every inflected form to the sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db21299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiws(pos_path, neg_path, encoding=\"utf-8-sig\"):\n",
    "    \"\"\"\n",
    "    Read both positive and negative SentiWS files and\n",
    "    return a dict {word_or_form_lower: score}, where\n",
    "    any word appearing in both files gets the average of the two scores.\n",
    "    \"\"\"\n",
    "    lex = {}\n",
    "    \n",
    "    # 1) Load all positive words\n",
    "    with open(pos_path, encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Split on tabs: [\"Abmachung|NN\", \"0.0040\", \"Abmachungen\"]\n",
    "            parts = line.split(\"\\t\")\n",
    "            base_pos = parts[0]\n",
    "            score = float(parts[1].replace(\",\", \".\"))\n",
    "            base = base_pos.split(\"|\")[0].lower()\n",
    "            # Base form\n",
    "            lex[base] = score\n",
    "            # Each inflected form\n",
    "            if len(parts) >= 3 and parts[2].strip():\n",
    "                for form in parts[2].split(\",\"):\n",
    "                    lex[form.lower()] = score\n",
    "                    \n",
    "    # 2) Load all negative words\n",
    "    with open(neg_path, encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Split on tabs: [\"Abmachung|NN\", \"0.0040\", \"Abmachungen\"]\n",
    "            parts = line.split(\"\\t\")\n",
    "            base_neg = parts[0]\n",
    "            score = float(parts[1].replace(\",\", \".\"))\n",
    "            base = base_neg.split(\"|\")[0].lower()\n",
    "            \n",
    "            # Helper to merge into lex\n",
    "            def merge(word):\n",
    "                w = word.lower()\n",
    "                if w in lex:\n",
    "                    lex[w] = (lex[w] + score) / 2.0\n",
    "                else:\n",
    "                    lex[w] = score\n",
    "\n",
    "            merge(base)\n",
    "            \n",
    "            # Each inflected form\n",
    "            if len(parts) >= 3 and parts[2].strip():\n",
    "                for form in parts[2].split(\",\"):\n",
    "                    merge(form)       \n",
    "            \n",
    "    return lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55b4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load lexicon\n",
    "lex = load_sentiws(\"SentiWS_v2.0_Positive.txt\",\n",
    "                   \"SentiWS_v2.0_Negative.txt\")\n",
    "vocab = list(lex.keys())\n",
    "scores = np.array([lex[w] for w in vocab], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25e630",
   "metadata": {},
   "source": [
    "Then I calculate the sum of sentiment scores for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8148839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:26.945552\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import score_chunk\n",
    "from datetime import datetime\n",
    "\n",
    "# Number of cores to use\n",
    "NUM_CORE = mp.cpu_count() - 4 \n",
    "\n",
    "# Split data into chunks for parallel processing\n",
    "chunk_size = len(data.texts) // NUM_CORE + 1 \n",
    "text_chunks = [data.texts[i:i + chunk_size] for i in range(0, len(data.texts), chunk_size)]\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "    # Process each chunk in parallel\n",
    "    scr_sum_results = pool.starmap(score_chunk.score_chunk, [(chunk, vocab, scores) for chunk in text_chunks])\n",
    "\n",
    "    # Close and join the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine results from all chunks\n",
    "    scr_sum = np.concatenate(scr_sum_results)\n",
    "\n",
    "print(datetime.now() - startTime)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0df404",
   "metadata": {},
   "source": [
    "Next, I calculate the total number of words in each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d602a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:38.211132\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../uncertainty\")\n",
    "\n",
    "startTime = datetime.now() \n",
    "\n",
    "# Import the function calculating the number of words in a text\n",
    "import count_words_mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeddbfe",
   "metadata": {},
   "source": [
    "Finally, I calculate the average sentiment score per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7647b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average sentiment per word\n",
    "# avoid division-by-zero\n",
    "avg_sent = np.zeros_like(scr_sum, dtype=float)\n",
    "count_results = np.array(count_results)\n",
    "mask = count_results > 0\n",
    "avg_sent[mask] = scr_sum[mask] / count_results[mask]\n",
    "\n",
    "# Attach sentiment to the DataFrame\n",
    "data[\"sentiment\"] = avg_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcba0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>-0.009476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>-0.015265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>0.000781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sperrfrist 1. Januar 1000 HBV fordert umfassen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>-0.004944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schamir weist Nahost-Äußerungen des neuen EG-P...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>0.002576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  day  month  year  \\\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...    1      1  1991   \n",
       "1  Welajati: Iran bleibt bei einem Krieg am Golf ...    1      1  1991   \n",
       "2  Bush will offenbar seinen Außenminister erneut...    1      1  1991   \n",
       "3  Sperrfrist 1. Januar 1000 HBV fordert umfassen...    1      1  1991   \n",
       "4  Schamir weist Nahost-Äußerungen des neuen EG-P...    1      1  1991   \n",
       "\n",
       "   sentiment  \n",
       "0  -0.009476  \n",
       "1  -0.015265  \n",
       "2   0.000781  \n",
       "3  -0.004944  \n",
       "4   0.002576  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a45bc",
   "metadata": {},
   "source": [
    "Now, I incorporate the topic distributions for each article, which were previously computed using the Latent Dirichlet Allocation (LDA) algorithm in the notebook titled `Topic Model Estimation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423dd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path variable to point to the 'topics' directory.\n",
    "path = os.getcwd().replace('\\\\sentiment', '\\\\topics')\n",
    "\n",
    "# Load the article topics from a CSV file\n",
    "article_topics = pd.read_csv(path + '\\\\article_topic.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "# Merge the `data` DataFrame with the `article_topics` DataFrame\n",
    "data = pd.concat([data, article_topics], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2c8fc",
   "metadata": {},
   "source": [
    "I define a function, `get_average_sentiment`, which calculates the average sentiment measure for each topic on a given day. This function selects a specified number of articles with the highest proportions for each topic and computes the average sentiment across these articles. The `calculate_average_sentiment` function applies this calculation to the entire dataset by grouping the data by date and processing each group in parallel using multiprocessing. The results are then combined into a single pandas DataFrame, with the topics as columns and the dates as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357a15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_average_sentiment import get_average_sentiment\n",
    "\n",
    "# Convert 'year', 'month', 'day' to datetime\n",
    "data['date'] = pd.to_datetime(data[['year', 'month', 'day']])\n",
    "\n",
    "def calculate_average_sentiment(data, n_articles):\n",
    "    \"\"\"\n",
    "    Function to calculate average sentiment for topics based on a specified number of articles.\n",
    "    \"\"\"\n",
    "    # Group data by 'date' to ensure each day stays together\n",
    "    grouped_by_date = [group for _, group in data.groupby('date')]\n",
    "    \n",
    "    # Prepare arguments for starmap (pair each group with the value of n_articles)\n",
    "    args = [(group, n_articles) for group in grouped_by_date]\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        # Create a multiprocessing pool\n",
    "        pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "        # Process each group (one day of data) in parallel\n",
    "        results = pool.starmap(get_average_sentiment, args)\n",
    "\n",
    "        # Concatenate the results into a single DataFrame\n",
    "        daily_average_sentiment = pd.concat(results)\n",
    "\n",
    "        # Close and join the pool\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    return daily_average_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5806378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:47.102173\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() \n",
    "\n",
    "# Generate average sentiment for 10 articles\n",
    "daily_average_sentiment_10 = calculate_average_sentiment(data, n_articles=10)\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c257c6af",
   "metadata": {},
   "source": [
    "Now I am going to load the daily topics and adjust them using the DataFrame `daily_average_sentiment_10`. The result is a dataframe where topic distributions are multiplied with the average sentiment of each topic for a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e22bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the daily topics from a CSV file\n",
    "daily_topics = pd.read_csv(path + '\\\\daily_topics.csv', encoding='utf-8')\n",
    "\n",
    "# Convert year, month, and day into a single date column\n",
    "daily_topics['date'] = pd.to_datetime(daily_topics[['year','month','day']])\n",
    "daily_topics.drop(columns=['year', 'month', 'day'], inplace=True)\n",
    "\n",
    "# Now, set 'date' as index\n",
    "daily_topics.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8724d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment adjustment to the daily topics\n",
    "sentiment_adjusted_daily_topics = daily_topics.multiply(daily_average_sentiment_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d694a",
   "metadata": {},
   "source": [
    "I iterate over each topic and generate a graph comparing the original and sentiment-adjusted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4147d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "\n",
    "# Create a directory to save the plots\n",
    "os.makedirs('topics_plots_SentiWS', exist_ok=True)\n",
    "\n",
    "# Define the shaded areas for recessions\n",
    "recessions = [\n",
    "    (\"1992-01-01\", \"1993-12-31\"),  # Post-reunification recession\n",
    "    (\"2001-01-01\", \"2001-12-31\"),  # Dot-com recession\n",
    "    (\"2008-01-01\", \"2009-12-31\"),  # Great Recession\n",
    "    (\"2011-01-01\", \"2013-12-31\")   # European sovereign debt crisis\n",
    "]\n",
    "\n",
    "# Calculate the 180-day rolling mean for each series\n",
    "daily_topics_rm = daily_topics.rolling(window=180).mean()\n",
    "sentiment_adjusted_daily_topics_10_rm = sentiment_adjusted_daily_topics.rolling(window=180).mean()\n",
    "\n",
    "# Iterate over each topic\n",
    "for i in range(daily_topics.shape[1]):\n",
    "    # Generate the plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original topics on the primary y-axis\n",
    "    ax1.plot(daily_topics_rm.index, daily_topics_rm.iloc[:, i], label='Original Topic', color='black')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Original Topic Proportion', color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Add shaded areas for recessions\n",
    "    for start, end in recessions:\n",
    "        ax1.axvspan(pd.to_datetime(start), pd.to_datetime(end), color='grey', alpha=0.3)\n",
    "    \n",
    "    # Create a secondary y-axis for the sentiment-adjusted topics\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(sentiment_adjusted_daily_topics_10_rm.index, sentiment_adjusted_daily_topics_10_rm.iloc[:, i], label='Sentiment-Adjusted Topic', linestyle='--')\n",
    "    ax2.set_ylabel('Sentiment-Adjusted Topic')\n",
    "    ax2.tick_params(axis='y')\n",
    "    \n",
    "    # Add title and legends\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Format the x-axis to show every year\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Save the plot in the 'topics_plots' directory\n",
    "    plt.savefig('topics_plots_SentiWS/Topic_' + str(i) + '.png')\n",
    "    \n",
    "    # Clear the current figure to free memory\n",
    "    plt.clf()\n",
    "    \n",
    "    # Close the current figure to free memory\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff5cc81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "sentiment_adjusted_daily_topics = sentiment_adjusted_daily_topics.reset_index()\n",
    "\n",
    "# Create 'year', 'month', and 'day' columns\n",
    "sentiment_adjusted_daily_topics['year'] = sentiment_adjusted_daily_topics['date'].dt.year\n",
    "sentiment_adjusted_daily_topics['month'] = sentiment_adjusted_daily_topics['date'].dt.month\n",
    "sentiment_adjusted_daily_topics['day'] = sentiment_adjusted_daily_topics['date'].dt.day\n",
    "\n",
    "# Drop the old 'index' column which holds the date\n",
    "sentiment_adjusted_daily_topics = sentiment_adjusted_daily_topics.drop(columns=['date'])\n",
    "\n",
    "# Reorder the columns to have 'year', 'month', 'day' as the first three columns\n",
    "cols = ['year', 'month', 'day'] + [col for col in sentiment_adjusted_daily_topics if col not in ['year', 'month', 'day']]\n",
    "sentiment_adjusted_daily_topics_format = sentiment_adjusted_daily_topics[cols]\n",
    "\n",
    "# Save sentiment-adjusted topics to a CSV file\n",
    "sentiment_adjusted_daily_topics_format.to_csv('sentiment_adjusted_daily_topics_SentiWS.csv', encoding='utf-8', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env_gpu",
   "language": "python",
   "name": "py3_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
