{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b148d4c",
   "metadata": {},
   "source": [
    "In this notebook, I will identify all uncertainty-related terms in our corpus and calculate their frequencies. First, I load the datasets from Handelsblatt, SZ, Welt, and dpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6bc882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3336299\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sperrfrist 1. Januar 1000 HBV fordert umfassen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schamir weist Nahost-Äußerungen des neuen EG-P...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  day  month  year\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...    1      1  1991\n",
       "1  Welajati: Iran bleibt bei einem Krieg am Golf ...    1      1  1991\n",
       "2  Bush will offenbar seinen Außenminister erneut...    1      1  1991\n",
       "3  Sperrfrist 1. Januar 1000 HBV fordert umfassen...    1      1  1991\n",
       "4  Schamir weist Nahost-Äußerungen des neuen EG-P...    1      1  1991"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Set the path variable to point to the 'newspaper_data_processing' directory.\n",
    "path = os.getcwd().replace('\\\\nowcasting_with_text\\\\uncertainty', '\\\\newspaper_data_processing')\n",
    "\n",
    "# Load pre-processed 'dpa' dataset from a CSV file.\n",
    "dpa = pd.read_csv(path + '\\\\dpa\\\\' + 'dpa_prepro_final.csv', encoding = 'utf-8', sep=';', index_col = 0,  keep_default_na=False,\n",
    "                   dtype = {'rubrics': 'str', \n",
    "                            'source': 'str',\n",
    "                            'keywords': 'str',\n",
    "                            'title': 'str',\n",
    "                            'city': 'str',\n",
    "                            'genre': 'str',\n",
    "                            'wordcount': 'str'},\n",
    "                  converters = {'paragraphs': literal_eval})\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "dpa = dpa[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'SZ' dataset from a CSV file.\n",
    "sz = pd.read_csv(path + '\\\\SZ\\\\' + 'sz_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'newspaper_2': 'str',\n",
    "                                                                                                 'quelle_texts': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "sz.page = sz.page.fillna('')\n",
    "sz.newspaper = sz.newspaper.fillna('')\n",
    "sz.newspaper_2 = sz.newspaper_2.fillna('')\n",
    "sz.rubrics = sz.rubrics.fillna('')\n",
    "sz.quelle_texts = sz.quelle_texts.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "sz = sz[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Handelsblatt' dataset from a CSV file.\n",
    "hb = pd.read_csv(path + '\\\\Handelsblatt\\\\' + 'hb_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'kicker': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'series_title': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "hb.page = hb.page.fillna('')\n",
    "hb.series_title = hb.series_title.fillna('')\n",
    "hb.kicker = hb.kicker.fillna('')\n",
    "hb.rubrics = hb.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "hb = hb[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Welt' dataset from a CSV file.\n",
    "welt = pd.read_csv(path + '\\\\Welt\\\\' + 'welt_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'rubrics': 'str',\n",
    "                                                                                                 'title': 'str'})\n",
    "welt.title = welt.title.fillna('')\n",
    "welt.rubrics = welt.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "welt = welt[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Concatenate the 'dpa', 'sz', 'hb', and 'welt' DataFrames into a single DataFrame 'data'.\n",
    "data = pd.concat([dpa, sz, hb, welt])\n",
    "\n",
    "# The number of articles in the final dataset.\n",
    "print(len(data))\n",
    "\n",
    "# Sort the data in chronological order.\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "# Reset the index of the DataFrame\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7db4d3",
   "metadata": {},
   "source": [
    "I utilize a function, `find_unsicher`, to search through all the texts in the corpus for occurrences of words containing the substring \"unsicher\". I systematically lowercase each text to ensure the search is case-insensitive, then employ a regular expression to identify and extract these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601e6954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:20.970416\n"
     ]
    }
   ],
   "source": [
    "# Use multiprocessing module for parallel computing\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Set the number of cores to use\n",
    "#NUM_CORE = mp.cpu_count()-4\n",
    "NUM_CORE = 30\n",
    "\n",
    "import find_unsicher\n",
    "\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    list_of_results = pool.map(find_unsicher.find_unsicher, [text for text in data.texts])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab235030",
   "metadata": {},
   "source": [
    "Now I extract unique terms that contain the substring \"unsicher\" and save them to a text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077e4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n"
     ]
    }
   ],
   "source": [
    "uncertainty_terms = set([item for sublist in list_of_results for item in sublist])\n",
    "print(len(uncertainty_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32cf6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data to be written to a text file\n",
    "uncertainty_terms_data = \"\\n\".join(sorted(uncertainty_terms))\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'uncertainty_terms.txt'\n",
    "\n",
    "# Write the data to the file with UTF-8 encoding\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(uncertainty_terms_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f28dd7",
   "metadata": {},
   "source": [
    "To understand the frequency of various uncertainty terms across a corpus, I compute the occurrence count of each term from the `uncertainty_terms` set within individual texts, producing dictionaries where the keys are words from `uncertainty_terms` and the values represent their respective counts across the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135b8fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:02:11.720714\n"
     ]
    }
   ],
   "source": [
    "import count_uncertainty_terms\n",
    "#NUM_CORE = mp.cpu_count()-4\n",
    "NUM_CORE = 60\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "# Create an iterable of tuples, each containing a text and the uncertainty_terms\n",
    "args = [(text, uncertainty_terms) for text in data.texts]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    list_of_counts = pool.starmap(count_uncertainty_terms.count_uncertainty_terms, args)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b6771",
   "metadata": {},
   "source": [
    "Next, I aggregate and sort the occurrence counts of uncertainty terms across the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b631653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate counts across the entire corpus\n",
    "aggregate_counts = {}\n",
    "for count_dict in list_of_counts:\n",
    "    for term, count in count_dict.items():\n",
    "        if term in aggregate_counts:\n",
    "            aggregate_counts[term] += count\n",
    "        else:\n",
    "            aggregate_counts[term] = count\n",
    "\n",
    "# Convert the aggregate counts dictionary to a list of tuples and sort by count\n",
    "sorted_counts = sorted(aggregate_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7bcd15",
   "metadata": {},
   "source": [
    "Upon analyzing the corpus for uncertainty-related terms and their frequencies, I identify two distinct categories: general uncertainty terms (e.g., \"Unsicherheit,\" \"Verunsicherung\", or \"unsicher\") and German compounds including the concept of uncertainty (e.g., \"Unsicherheitsfaktor,\" \"Rechtsunsicherheit\", or \"Zinsunsicherheit\"). I save the sorted list `sorted_counts` to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3dafea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# The filename for a CSV file\n",
    "filename = \"sorted_counts.csv\"\n",
    "\n",
    "# Open the file in write mode and specify newline to prevent extra blank lines\n",
    "with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    # Create a csv.writer object\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header row\n",
    "    writer.writerow([\"Word\", \"Count\"])\n",
    "    # Write the data rows\n",
    "    for item in sorted_counts:\n",
    "        writer.writerow(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835779d2",
   "metadata": {},
   "source": [
    "This code offers a faster alternative for calculating frequencies of all uncertainty terms across a corpus with scikit-learn's `CountVectorizer`, yielding identical results to the `count_uncertainty_terms` function but more efficiently, thanks to scikit-learn's optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72dd0830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:23:01.312013\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Convert the set of uncertainty terms to a list to use as vocabulary for the CountVectorizer\n",
    "vocabulary = list(uncertainty_terms)\n",
    "\n",
    "# Initialize CountVectorizer with the vocabulary of uncertainty terms\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "# Fit the vectorizer to the texts and transform the texts into a term frequency matrix\n",
    "X = vectorizer.fit_transform(data.texts)\n",
    "\n",
    "# Sum the occurrences of each term across all documents to get the total counts\n",
    "term_frequencies = np.sum(X, axis=0)\n",
    "\n",
    "# Convert the matrix to a flat array and then to a list for easier handling\n",
    "term_frequencies = np.squeeze(np.asarray(term_frequencies)).tolist()\n",
    "\n",
    "# Map the term frequencies back to the corresponding terms\n",
    "term_frequency_dict = dict(zip(vectorizer.get_feature_names(), term_frequencies))\n",
    "\n",
    "# Sort the dictionary by frequency\n",
    "sorted_term_frequency = sorted(term_frequency_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c264f4",
   "metadata": {},
   "source": [
    "This is to show that results are indeed identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7afc183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionaries are identical.\n"
     ]
    }
   ],
   "source": [
    "are_identical = dict(sorted_counts) == dict(sorted_term_frequency)\n",
    "\n",
    "if are_identical:\n",
    "    print(\"The dictionaries are identical.\")\n",
    "else:\n",
    "    print(\"The dictionaries are not identical.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env_gpu",
   "language": "python",
   "name": "py3_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
