{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84bd8f6c",
   "metadata": {},
   "source": [
    "In this notebook, I focus on creating uncertainty-adjusted topic series by following a clear methodology. For each day, I start by identifying the 10 articles with the highest proportion of each topic. Next, I calculate the average uncertainty measure for these articles, representing the topic's uncertainty for that day. Finally, I adjust the daily topic values by multiplying them with the corresponding uncertainty measure.\n",
    "\n",
    "First, I load the datasets from Handelsblatt, SZ, Welt, and dpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e617bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3336299\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sperrfrist 1. Januar 1000 HBV fordert umfassen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schamir weist Nahost-Äußerungen des neuen EG-P...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  day  month  year\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...    1      1  1991\n",
       "1  Welajati: Iran bleibt bei einem Krieg am Golf ...    1      1  1991\n",
       "2  Bush will offenbar seinen Außenminister erneut...    1      1  1991\n",
       "3  Sperrfrist 1. Januar 1000 HBV fordert umfassen...    1      1  1991\n",
       "4  Schamir weist Nahost-Äußerungen des neuen EG-P...    1      1  1991"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Set the path variable to point to the 'newspaper_data_processing' directory.\n",
    "path = os.getcwd().replace('\\\\nowcasting_with_text\\\\uncertainty', '\\\\newspaper_data_processing')\n",
    "\n",
    "# Load pre-processed 'dpa' dataset from a CSV file.\n",
    "dpa = pd.read_csv(path + '\\\\dpa\\\\' + 'dpa_prepro_final.csv', encoding = 'utf-8', sep=';', index_col = 0,  keep_default_na=False,\n",
    "                   dtype = {'rubrics': 'str', \n",
    "                            'source': 'str',\n",
    "                            'keywords': 'str',\n",
    "                            'title': 'str',\n",
    "                            'city': 'str',\n",
    "                            'genre': 'str',\n",
    "                            'wordcount': 'str'},\n",
    "                  converters = {'paragraphs': literal_eval})\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "dpa = dpa[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'SZ' dataset from a CSV file.\n",
    "sz = pd.read_csv(path + '\\\\SZ\\\\' + 'sz_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'newspaper_2': 'str',\n",
    "                                                                                                 'quelle_texts': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "sz.page = sz.page.fillna('')\n",
    "sz.newspaper = sz.newspaper.fillna('')\n",
    "sz.newspaper_2 = sz.newspaper_2.fillna('')\n",
    "sz.rubrics = sz.rubrics.fillna('')\n",
    "sz.quelle_texts = sz.quelle_texts.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "sz = sz[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Handelsblatt' dataset from a CSV file.\n",
    "hb = pd.read_csv(path + '\\\\Handelsblatt\\\\' + 'hb_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'kicker': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'series_title': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "hb.page = hb.page.fillna('')\n",
    "hb.series_title = hb.series_title.fillna('')\n",
    "hb.kicker = hb.kicker.fillna('')\n",
    "hb.rubrics = hb.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "hb = hb[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Welt' dataset from a CSV file.\n",
    "welt = pd.read_csv(path + '\\\\Welt\\\\' + 'welt_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'rubrics': 'str',\n",
    "                                                                                                 'title': 'str'})\n",
    "welt.title = welt.title.fillna('')\n",
    "welt.rubrics = welt.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "welt = welt[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Concatenate the 'dpa', 'sz', 'hb', and 'welt' DataFrames into a single DataFrame 'data'.\n",
    "data = pd.concat([dpa, sz, hb, welt])\n",
    "\n",
    "# The number of articles in the final dataset.\n",
    "print(len(data))\n",
    "\n",
    "# Sort the data in chronological order.\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "# Reset the index of the DataFrame\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce852620",
   "metadata": {},
   "source": [
    "Next, I calculate the uncertainty measure for each article by determining the ratio of uncertainty-related words to the total number of words in the article. This measure is derived from a set of 46 uncertainty terms that appeared in the corpus at least 20 times. These terms include general words like \"Unsicherheit\" (uncertainty) and compound terms such as \"Unsicherheitsfaktor\" (uncertainty factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6eb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"sorted_counts.csv\")\n",
    "\n",
    "# Filter words with count >= 20\n",
    "uncertainty_terms = df.loc[df['Count'] >= 20, 'Word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133bad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:59.225366\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import count_uncertainty_terms_chunk\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Number of cores to use\n",
    "NUM_CORE = mp.cpu_count() - 4 \n",
    "\n",
    "# Split data into chunks for parallel processing\n",
    "chunk_size = len(data.texts) // NUM_CORE + 1 \n",
    "text_chunks = [data.texts[i:i + chunk_size] for i in range(0, len(data.texts), chunk_size)]\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "    # Process each chunk in parallel\n",
    "    uncertainty_results = pool.starmap(count_uncertainty_terms_chunk.count_uncertainty_terms_chunk, [(chunk, uncertainty_terms) for chunk in text_chunks])\n",
    "\n",
    "    # Close and join the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine results from all chunks\n",
    "    uncertainty_counts = np.concatenate(uncertainty_results)\n",
    "\n",
    "print(datetime.now() - startTime)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f61cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:38.902484\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() \n",
    "\n",
    "# Import the function calculating the number of words in a text\n",
    "import count_words_mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c81531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a list to NumPy array\n",
    "count_results = np.array(count_results)\n",
    "\n",
    "# Perform element-wise division\n",
    "uncertainty_measure = np.divide(uncertainty_counts, count_results, out=np.zeros_like(uncertainty_counts, dtype=float), \n",
    "                                where=count_results != 0)\n",
    "\n",
    "data['uncertainty'] = uncertainty_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062f76e",
   "metadata": {},
   "source": [
    "Now, I incorporate the topic distributions for each article, which were previously computed using the Latent Dirichlet Allocation (LDA) algorithm in the notebook titled `Topic Model Estimation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a21a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path variable to point to the 'topics' directory.\n",
    "path = os.getcwd().replace('\\\\uncertainty', '\\\\topics')\n",
    "\n",
    "# Load the article topics from a CSV file\n",
    "article_topics = pd.read_csv(path + '\\\\article_topic.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "# Merge the `data` DataFrame with the `article_topics` DataFrame\n",
    "data = pd.concat([data, article_topics], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a35a4a",
   "metadata": {},
   "source": [
    "I define a function, `get_average_uncertainty`, which calculates the average uncertainty measure for each topic on a given day. This function selects a specified number of articles with the highest proportions for each topic and computes the average uncertainty across these articles. The `calculate_average_uncertainty` function applies this calculation to the entire dataset by grouping the data by date and processing each group in parallel using multiprocessing. The results are then combined into a single pandas DataFrame, with the topics as columns and the dates as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0113ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_average_uncertainty import get_average_uncertainty\n",
    "\n",
    "# Convert 'year', 'month', 'day' to datetime\n",
    "data['date'] = pd.to_datetime(data[['year', 'month', 'day']])\n",
    "\n",
    "def calculate_average_uncertainty(data, n_articles):\n",
    "    \"\"\"\n",
    "    Function to calculate average uncertainty for topics based on a specified number of articles.\n",
    "    \"\"\"\n",
    "    # Group data by 'date' to ensure each day stays together\n",
    "    grouped_by_date = [group for _, group in data.groupby('date')]\n",
    "    \n",
    "    # Prepare arguments for starmap (pair each group with the value of n_articles)\n",
    "    args = [(group, n_articles) for group in grouped_by_date]\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        # Create a multiprocessing pool\n",
    "        pool = mp.Pool(NUM_CORE)\n",
    "\n",
    "        # Process each group (one day of data) in parallel\n",
    "        results = pool.starmap(get_average_uncertainty, args)\n",
    "\n",
    "        # Concatenate the results into a single DataFrame\n",
    "        daily_average_uncertainty = pd.concat(results)\n",
    "\n",
    "        # Close and join the pool\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    return daily_average_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9bf3d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:09:53.736113\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() \n",
    "\n",
    "# Generate average uncertainty for 5, 10, and 15 articles\n",
    "daily_average_uncertainty_5 = calculate_average_uncertainty(data, n_articles=5)\n",
    "daily_average_uncertainty_10 = calculate_average_uncertainty(data, n_articles=10)\n",
    "daily_average_uncertainty_15 = calculate_average_uncertainty(data, n_articles=15)\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6e708",
   "metadata": {},
   "source": [
    "Now I am going to load the daily topics and adjust them using the DataFrame `daily_average_uncertainty_10`. The result is a dataframe where topic distributions are multiplied with the average uncertainty of each topic for a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a73bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the daily topics from a CSV file\n",
    "daily_topics = pd.read_csv(path + '\\\\daily_topics.csv', encoding='utf-8')\n",
    "\n",
    "# Convert year, month, and day into a single date column\n",
    "daily_topics['date'] = pd.to_datetime(daily_topics[['year','month','day']])\n",
    "daily_topics.drop(columns=['year', 'month', 'day'], inplace=True)\n",
    "\n",
    "# Now, set 'date' as index\n",
    "daily_topics.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2194bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply uncertainty adjustment to the daily topics\n",
    "uncertainty_adjusted_daily_topics = daily_topics.multiply(daily_average_uncertainty_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37921e",
   "metadata": {},
   "source": [
    "I iterate over each topic and generate a graph comparing the original and uncertainty-adjusted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55d41adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "\n",
    "# Create a directory to save the plots\n",
    "os.makedirs('topics_plots', exist_ok=True)\n",
    "\n",
    "# Define the shaded areas for recessions\n",
    "recessions = [\n",
    "    (\"1992-01-01\", \"1993-12-31\"),  # Post-reunification recession\n",
    "    (\"2001-01-01\", \"2001-12-31\"),  # Dot-com recession\n",
    "    (\"2008-01-01\", \"2009-12-31\"),  # Great Recession\n",
    "    (\"2011-01-01\", \"2013-12-31\")   # European sovereign debt crisis\n",
    "]\n",
    "\n",
    "# Calculate the 180-day rolling mean for each series\n",
    "daily_topics_rm = daily_topics.rolling(window=180).mean()\n",
    "uncertainty_adjusted_daily_topics_10_rm = uncertainty_adjusted_daily_topics.rolling(window=180).mean()\n",
    "\n",
    "# Iterate over each topic\n",
    "for i in range(daily_topics.shape[1]):\n",
    "    # Generate the plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original topics on the primary y-axis\n",
    "    ax1.plot(daily_topics_rm.index, daily_topics_rm.iloc[:, i], label='Original Topic', color='black')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Original Topic Proportion', color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Add shaded areas for recessions\n",
    "    for start, end in recessions:\n",
    "        ax1.axvspan(pd.to_datetime(start), pd.to_datetime(end), color='grey', alpha=0.3)\n",
    "    \n",
    "    # Create a secondary y-axis for the uncertainty-adjusted topics\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(uncertainty_adjusted_daily_topics_10_rm.index, uncertainty_adjusted_daily_topics_10_rm.iloc[:, i], label='Uncertainty-Adjusted Topic', linestyle='--')\n",
    "    ax2.set_ylabel('Uncertainty-Adjusted Topic')\n",
    "    ax2.tick_params(axis='y')\n",
    "    \n",
    "    # Add title and legends\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Format the x-axis to show every year\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Save the plot in the 'topics_plots' directory\n",
    "    plt.savefig('topics_plots/Topic_' + str(i) + '.png')\n",
    "    \n",
    "    # Clear the current figure to free memory\n",
    "    plt.clf()\n",
    "    \n",
    "    # Close the current figure to free memory\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7621bd",
   "metadata": {},
   "source": [
    "To check the robustness of my results to the choice of the number of top articles used to determine uncertainty, I compare the uncertainty-adjusted topics for 5, 10, and 15 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2411ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the plots\n",
    "os.makedirs('topics_plots_number_of_articles', exist_ok=True)\n",
    "\n",
    "# Calculate the 180-day rolling mean for each series\n",
    "uncertainty_adjusted_daily_topics_5_rm = daily_topics.multiply(daily_average_uncertainty_5).rolling(window=180).mean()\n",
    "uncertainty_adjusted_daily_topics_15_rm = daily_topics.multiply(daily_average_uncertainty_15).rolling(window=180).mean()\n",
    "\n",
    "# Iterate over each topic\n",
    "for i in range(daily_topics.shape[1]):\n",
    "    # Generate the plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original topics on the primary y-axis\n",
    "    ax1.plot(daily_topics_rm.index, daily_topics_rm.iloc[:, i], label='Original Topic', color='black')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Original Topic Proportion', color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Add shaded areas for recessions\n",
    "    for start, end in recessions:\n",
    "        ax1.axvspan(pd.to_datetime(start), pd.to_datetime(end), color='grey', alpha=0.3)\n",
    "    \n",
    "    # Create a secondary y-axis for the uncertainty-adjusted topics\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(uncertainty_adjusted_daily_topics_5_rm.index, uncertainty_adjusted_daily_topics_5_rm.iloc[:, i], label='Uncertainty-Adjusted (Top 5 Articles)', linestyle='--', color='blue')\n",
    "    ax2.plot(uncertainty_adjusted_daily_topics_10_rm.index, uncertainty_adjusted_daily_topics_10_rm.iloc[:, i], label='Uncertainty-Adjusted (Top 10 Articles)', linestyle='-.', color='green')\n",
    "    ax2.plot(uncertainty_adjusted_daily_topics_15_rm.index, uncertainty_adjusted_daily_topics_15_rm.iloc[:, i], label='Uncertainty-Adjusted (Top 15 Articles)', linestyle=':', color='red')\n",
    "    ax2.set_ylabel('Uncertainty-Adjusted Topic')\n",
    "    ax2.tick_params(axis='y')\n",
    "    \n",
    "    # Add title and legends\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Format the x-axis to show every year\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Save the plot in the 'topics_plots' directory\n",
    "    plt.savefig('topics_plots_number_of_articles/Topic_' + str(i) + '.png')\n",
    "    \n",
    "    # Clear the current figure to free memory\n",
    "    plt.clf()\n",
    "    \n",
    "    # Close the current figure to free memory\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea471062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "uncertainty_adjusted_daily_topics = uncertainty_adjusted_daily_topics.reset_index()\n",
    "\n",
    "# Create 'year', 'month', and 'day' columns\n",
    "uncertainty_adjusted_daily_topics['year'] = uncertainty_adjusted_daily_topics['date'].dt.year\n",
    "uncertainty_adjusted_daily_topics['month'] = uncertainty_adjusted_daily_topics['date'].dt.month\n",
    "uncertainty_adjusted_daily_topics['day'] = uncertainty_adjusted_daily_topics['date'].dt.day\n",
    "\n",
    "# Drop the old 'index' column which holds the date\n",
    "uncertainty_adjusted_daily_topics = uncertainty_adjusted_daily_topics.drop(columns=['date'])\n",
    "\n",
    "# Reorder the columns to have 'year', 'month', 'day' as the first three columns\n",
    "cols = ['year', 'month', 'day'] + [col for col in uncertainty_adjusted_daily_topics if col not in ['year', 'month', 'day']]\n",
    "uncertainty_adjusted_daily_topics_format = uncertainty_adjusted_daily_topics[cols]\n",
    "\n",
    "# Save uncertainty-adjusted topics to a CSV file\n",
    "uncertainty_adjusted_daily_topics_format.to_csv('uncertainty_adjusted_daily_topics.csv', encoding='utf-8', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env_gpu",
   "language": "python",
   "name": "py3_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
